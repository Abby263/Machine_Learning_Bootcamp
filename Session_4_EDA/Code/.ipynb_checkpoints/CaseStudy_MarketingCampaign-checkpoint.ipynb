{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\imp.py\", line 297, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nModuleNotFoundError: No module named '_pywrap_tensorflow'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_pywrap_tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# use `dlopen()` for dynamic loading.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-74438b46370d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfancyimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchi2_contingency\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fancyimpute\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnuclear_norm_minimization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNuclearNormMinimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0miterative_imputer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIterativeImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmatrix_factorization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatrixFactorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0miterative_svd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIterativeSVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msimple_fill\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleFill\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fancyimpute\\matrix_factorization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mexit\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrelaunch\u001b[0m \u001b[0myour\u001b[0m \u001b[0mpython\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m from there.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\imp.py\", line 297, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\ABHAY RAJ SINGH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nModuleNotFoundError: No module named '_pywrap_tensorflow'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there."
     ]
    }
   ],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import KNN   \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "from random import randrange, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fancyimpute\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from fancyimpute) (0.19.2)\n",
      "Collecting cvxpy>=1.0.6 (from fancyimpute)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from fancyimpute) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\abhay raj singh\\appdata\\roaming\\python\\python37\\site-packages (from fancyimpute) (1.15.4)\n",
      "Requirement already satisfied: knnimpute in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from fancyimpute) (0.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from fancyimpute) (1.1.0)\n",
      "Collecting np-utils (from fancyimpute)\n",
      "Requirement already satisfied: six in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from fancyimpute) (1.11.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from fancyimpute) (0.12.0)\n",
      "Requirement already satisfied: osqp in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (0.4.1)\n",
      "Requirement already satisfied: fastcache in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (1.0.2)\n",
      "Collecting ecos>=2 (from cvxpy>=1.0.6->fancyimpute)\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/3a/59aa93b573a22fda44402383aeddcc2a081c31e61080af3da9d11855c77a/ecos-2.0.7.post1.tar.gz\n",
      "Collecting scs>=1.1.3 (from cvxpy>=1.0.6->fancyimpute)\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/fd/6e01c4f4a69fcc6c3db130ba55572089e78e77ea8c0921a679f9da1ec04c/scs-2.0.2.tar.gz\n",
      "Collecting multiprocess (from cvxpy>=1.0.6->fancyimpute)\n",
      "Requirement already satisfied: toolz in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (0.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (1.0.6)\n",
      "Requirement already satisfied: h5py in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (1.0.5)\n",
      "Requirement already satisfied: future>=0.16 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from np-utils->fancyimpute) (0.17.1)\n",
      "Requirement already satisfied: protobuf==3.1.0 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (3.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (0.31.1)\n",
      "Collecting dill>=0.2.8.1 (from multiprocess->cvxpy>=1.0.6->fancyimpute)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhay raj singh\\anaconda3\\lib\\site-packages (from protobuf==3.1.0->tensorflow->fancyimpute) (40.6.2)\n",
      "Collecting pyreadline>=1.7.1 (from dill>=0.2.8.1->multiprocess->cvxpy>=1.0.6->fancyimpute)\n",
      "Building wheels for collected packages: ecos, scs\n",
      "  Running setup.py bdist_wheel for ecos: started\n",
      "  Running setup.py bdist_wheel for ecos: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ABHAY RAJ SINGH\\AppData\\Local\\pip\\Cache\\wheels\\4b\\7d\\90\\39ff7dca0e5c06740afc993ab4209a1719fc1d616daf7af040\n",
      "  Running setup.py bdist_wheel for scs: started\n",
      "  Running setup.py bdist_wheel for scs: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ABHAY RAJ SINGH\\AppData\\Local\\pip\\Cache\\wheels\\ff\\f0\\aa\\530ccd478d7d9900b4e9ef5bc5a39e895ce110bed3d3ac653e\n",
      "Successfully built ecos scs\n",
      "Installing collected packages: ecos, scs, pyreadline, dill, multiprocess, cvxpy, np-utils, fancyimpute\n",
      "Successfully installed cvxpy-1.0.10 dill-0.2.8.2 ecos-2.0.7.post1 fancyimpute-0.4.2 multiprocess-0.70.6.1 np-utils-0.5.7.0 pyreadline-2.1 scs-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/16/da8cb8046149d50940c6110310983abb359bbb8cbc3539e6bef95c29428a/setuptools-40.6.2-py2.py3-none-any.whl (573kB)\n",
      "Installing collected packages: setuptools\n",
      "  Found existing installation: setuptools 40.2.0\n",
      "    Uninstalling setuptools-40.2.0:\n",
      "      Successfully uninstalled setuptools-40.2.0\n",
      "Successfully installed setuptools-40.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "os.chdir(\"C:/Users/ABHAY RAJ SINGH/Desktop/DS_Trainer/Session_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "marketing_train = pd.read_csv(\"marketing_tr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis\n",
    "marketing_train['schooling'] = marketing_train['schooling'].replace(\"illiterate\", \"unknown\")\n",
    "marketing_train['schooling'] = marketing_train['schooling'].replace([\"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"professional.course\"], \"high.school\")\n",
    "marketing_train['default'] = marketing_train['default'].replace(\"yes\", \"unknown\")\n",
    "marketing_train['marital'] = marketing_train['marital'].replace(\"unknown\", \"married\")\n",
    "marketing_train['month'] = marketing_train['month'].replace([\"sep\",\"oct\",\"mar\",\"dec\"], \"dec\")\n",
    "marketing_train['month'] = marketing_train['month'].replace([\"aug\",\"jul\",\"jun\",\"may\",\"nov\"], \"jun\")\n",
    "marketing_train['loan'] = marketing_train['loan'].replace(\"unknown\", \"no\")\n",
    "marketing_train['profession'] = marketing_train['profession'].replace([\"management\",\"unknown\",\"unemployed\",\"admin.\"], \"admin.\")\n",
    "marketing_train['profession'] = marketing_train['profession'].replace([\"blue-collar\",\"housemaid\",\"services\",\"self-employed\",\"entrepreneur\",\"technician\"], \"blue-collar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe with missing percentage\n",
    "missing_val = pd.DataFrame(marketing_train.isnull().sum())\n",
    "\n",
    "#Reset index\n",
    "missing_val = missing_val.reset_index()\n",
    "\n",
    "#Rename variable\n",
    "missing_val = missing_val.rename(columns = {'index': 'Variables', 0: 'Missing_percentage'})\n",
    "\n",
    "#Calculate percentage\n",
    "missing_val['Missing_percentage'] = (missing_val['Missing_percentage']/len(marketing_train))*100\n",
    "\n",
    "#descending order\n",
    "missing_val = missing_val.sort_values('Missing_percentage', ascending = False).reset_index(drop = True)\n",
    "\n",
    "#save output results \n",
    "missing_val.to_csv(\"Miising_perc.csv\", inex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imputation method\n",
    "#Actual value = 29\n",
    "#Mean = 40.01\n",
    "#Median = 38\n",
    "#KNN = 29.35\n",
    "\n",
    "#create missing value\n",
    "#marketing_train['custAge'].loc[70] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute with mean\n",
    "#marketing_train['custAge'] = marketing_train['custAge'].fillna(marketing_train['custAge'].mean())\n",
    "\n",
    "#Impute with median\n",
    "#marketing_train['custAge'] = marketing_train['custAge'].fillna(marketing_train['custAge'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN imputation\n",
    "#Assigning levels to the categories\n",
    "lis = []\n",
    "for i in range(0, marketing_train.shape[1]):\n",
    "    #print(i)\n",
    "    if(marketing_train.iloc[:,i].dtypes == 'object'):\n",
    "        marketing_train.iloc[:,i] = pd.Categorical(marketing_train.iloc[:,i])\n",
    "        #print(marketing_train[[i]])\n",
    "        marketing_train.iloc[:,i] = marketing_train.iloc[:,i].cat.codes \n",
    "        marketing_train.iloc[:,i] = marketing_train.iloc[:,i].astype('object')\n",
    "        \n",
    "        lis.append(marketing_train.columns[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace -1 with NA to impute\n",
    "for i in range(0, marketing_train.shape[1]):\n",
    "    marketing_train.iloc[:,i] = marketing_train.iloc[:,i].replace(-1, np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply KNN imputation algorithm\n",
    "marketing_train = pd.DataFrame(KNN(k = 3).complete(marketing_train), columns = marketing_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into proper datatypes\n",
    "for i in lis:\n",
    "    marketing_train.loc[:,i] = marketing_train.loc[:,i].round()\n",
    "    marketing_train.loc[:,i] = marketing_train.loc[:,i].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = marketing_train.copy()\n",
    "#marketing_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Plot boxplot to visualize Outliers\n",
    "# %matplotlib inline  \n",
    "# plt.boxplot(marketing_train['custAge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save numeric names\n",
    "cnames =  [\"custAge\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\",\n",
    "           \"nr.employed\", \"pmonths\", \"pastEmail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Detect and delete outliers from data\n",
    "# for i in cnames:\n",
    "#     print(i)\n",
    "#     q75, q25 = np.percentile(marketing_train.loc[:,i], [75 ,25])\n",
    "#     iqr = q75 - q25\n",
    "\n",
    "#     min = q25 - (iqr*1.5)\n",
    "#     max = q75 + (iqr*1.5)\n",
    "#     print(min)\n",
    "#     print(max)\n",
    "    \n",
    "#     marketing_train = marketing_train.drop(marketing_train[marketing_train.loc[:,i] < min].index)\n",
    "#     marketing_train = marketing_train.drop(marketing_train[marketing_train.loc[:,i] > max].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect and replace with NA\n",
    "# #Extract quartiles\n",
    "# q75, q25 = np.percentile(marketing_train['custAge'], [75 ,25])\n",
    "\n",
    "# #Calculate IQR\n",
    "# iqr = q75 - q25\n",
    "\n",
    "# #Calculate inner and outer fence\n",
    "# minimum = q25 - (iqr*1.5)\n",
    "# maximum = q75 + (iqr*1.5)\n",
    "\n",
    "# #Replace with NA\n",
    "# marketing_train.loc[marketing_train['custAge'] < minimum,:'custAge'] = np.nan\n",
    "# marketing_train.loc[marketing_train['custAge'] > maximum,:'custAge'] = np.nan\n",
    "\n",
    "# #Calculate missing value\n",
    "# missing_val = pd.DataFrame(marketing_train.isnull().sum())\n",
    "\n",
    "# #Impute with KNN\n",
    "# marketing_train = pd.DataFrame(KNN(k = 3).complete(marketing_train), columns = marketing_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correlation analysis\n",
    "#Correlation plot\n",
    "df_corr = marketing_train.loc[:,cnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the width and hieght of the plot\n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "#Generate correlation matrix\n",
    "corr = df_corr.corr()\n",
    "\n",
    "#Plot using seaborn library\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chisquare test of independence\n",
    "#Save categorical variables\n",
    "cat_names = [\"profession\", \"marital\", \"schooling\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day_of_week\", \"poutcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loop for chi square values\n",
    "for i in cat_names:\n",
    "    print(i)\n",
    "    chi2, p, dof, ex = chi2_contingency(pd.crosstab(marketing_train['responded'], marketing_train[i]))\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_train = marketing_train.drop(['pdays', 'emp.var.rate', 'day_of_week', 'loan', 'housing'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = marketing_train.copy()\n",
    "#marketing_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normality check\n",
    "%matplotlib inline  \n",
    "plt.hist(marketing_train['campaign'], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnames = [\"custAge\",\"campaign\",\"previous\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\",\n",
    "           \"pmonths\",\"pastEmail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nomalisation\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    marketing_train[i] = (marketing_train[i] - min(marketing_train[i]))/(max(marketing_train[i]) - min(marketing_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Standarisation\n",
    "# for i in cnames:\n",
    "#     print(i)\n",
    "#     marketing_train[i] = (marketing_train[i] - marketing_train[i].mean())/marketing_train[i].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simple random sampling\n",
    "#Sim_Sampling = marketing_train.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##Systematic Sampling\n",
    "# #Calculate the K value\n",
    "# k = len(marketing_train)/3500\n",
    "\n",
    "# # Generate a random number using simple random sampling\n",
    "# RandNum = randrange(0, 5)\n",
    "\n",
    "# #select Kth observation starting from RandNum\n",
    "# Sys_Sampling = marketing_train.iloc[RandNum::k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Stratified sampling\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# #Select categorical variable\n",
    "# y = marketing_train['profession']\n",
    "\n",
    "#select subset using stratified Sampling\n",
    "#Rest, Sample = train_test_split(marketing_train, test_size = 0.6, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marketing_train = pd.read_csv(\"marketing_train_Model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries for decision tree\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace target categories with Yes or No\n",
    "marketing_train['responded'] = marketing_train['responded'].replace(0, 'No')\n",
    "marketing_train['responded'] = marketing_train['responded'].replace(1, 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into train and test\n",
    "X = marketing_train.values[:, 0:16]\n",
    "Y = marketing_train.values[:,16]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "C50_model = tree.DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train)\n",
    "\n",
    "#predict new test cases\n",
    "C50_Predictions = C50_model.predict(X_test)\n",
    "\n",
    "#Create dot file to visualise tree  #http://webgraphviz.com/\n",
    "# dotfile = open(\"pt.dot\", 'w')\n",
    "# df = tree.export_graphviz(C50_model, out_file=dotfile, feature_names = marketing_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix \n",
    "# CM = confusion_matrix(y_test, y_pred)\n",
    "CM = pd.crosstab(y_test, C50_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "#(FN*100)/(FN+TP)\n",
    "\n",
    "#Results\n",
    "#Accuracy: 84.49\n",
    "#FNR: 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators = 20).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_Predictions = RF_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix \n",
    "# CM = confusion_matrix(y_test, y_pred)\n",
    "CM = pd.crosstab(y_test, RF_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "#(FN*100)/(FN+TP)\n",
    "\n",
    "#Accuracy: 88\n",
    "#FNR: 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us prepare data for logistic regression\n",
    "#replace target categories with Yes or No\n",
    "marketing_train['responded'] = marketing_train['responded'].replace('No', 0)\n",
    "marketing_train['responded'] = marketing_train['responded'].replace('Yes', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create logistic data. Save target variable first\n",
    "marketing_train_logit = pd.DataFrame(marketing_train['responded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add continous variables\n",
    "marketing_train_logit = marketing_train_logit.join(marketing_train[cnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_train_logit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create dummies for categorical variables\n",
    "cat_names = [\"profession\", \"marital\", \"schooling\", \"default\", \"contact\", \"month\", \"poutcome\"]\n",
    "\n",
    "for i in cat_names:\n",
    "    temp = pd.get_dummies(marketing_train[i], prefix = i)\n",
    "    marketing_train_logit = marketing_train_logit.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sample_Index = np.random.rand(len(marketing_train_logit)) < 0.8\n",
    "\n",
    "train = marketing_train_logit[Sample_Index]\n",
    "test = marketing_train_logit[~Sample_Index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select column indexes for independent variables\n",
    "train_cols = train.columns[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built Logistic Regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "logit = sm.Logit(train['responded'], train[train_cols]).fit()\n",
    "\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict test data\n",
    "test['Actual_prob'] = logit.predict(test[train_cols])\n",
    "\n",
    "test['ActualVal'] = 1\n",
    "test.loc[test.Actual_prob < 0.5, 'ActualVal'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build confusion matrix\n",
    "CM = pd.crosstab(test['responded'], test['ActualVal'])\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "(FN*100)/(FN+TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy: 90\n",
    "#FNR: 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = 9).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict test cases\n",
    "KNN_Predictions = KNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build confusion matrix\n",
    "CM = pd.crosstab(y_test, KNN_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "(FN*100)/(FN+TP)\n",
    "\n",
    "#Accuracy: 89\n",
    "#FNR: 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Naive Bayes implementation\n",
    "NB_model = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict test cases\n",
    "NB_Predictions = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build confusion matrix\n",
    "CM = pd.crosstab(y_test, NB_Predictions)\n",
    "\n",
    "#let us save TP, TN, FP, FN\n",
    "TN = CM.iloc[0,0]\n",
    "FN = CM.iloc[1,0]\n",
    "TP = CM.iloc[1,1]\n",
    "FP = CM.iloc[0,1]\n",
    "\n",
    "#check accuracy of model\n",
    "#accuracy_score(y_test, y_pred)*100\n",
    "#((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "\n",
    "#False Negative rate \n",
    "(FN*100)/(FN+TP)\n",
    "\n",
    "#Accuracy: 81\n",
    "#FNR: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Estimate optimum number of clusters\n",
    "cluster_range = range( 1, 20 )\n",
    "cluster_errors = []\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters).fit(df.iloc[:,0:4])\n",
    "    cluster_errors.append(clusters.inertia_)\n",
    "    \n",
    "#Create dataframe with cluster errors\n",
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot line chart to visualise number of clusters\n",
    "%matplotlib inline  \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement kmeans\n",
    "kmeans_model = KMeans(n_clusters = 3).fit(df.iloc[:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize output\n",
    "pd.crosstab(df['Species'], kmeans_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_range = range( 1, 20 )\n",
    "cluster_errors = []\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans( num_clusters )\n",
    "    clusters.fit( df.iloc[:,0:4] )\n",
    "    cluster_errors.append( clusters.inertia_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
